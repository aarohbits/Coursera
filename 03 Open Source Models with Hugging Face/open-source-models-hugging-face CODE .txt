

LESSON 2 


In the classroom, the libraries are already installed for you.
If you would like to run this code on your own machine, you can install the following:
  !pip install transformers
  
 
Build the chatbot pipeline using ðŸ¤— Transformers Library

Here is some code that suppresses warning messages.
	from transformers import pipeline
	
Define the conversation pipeline

	chatbot = pipeline(task="conversational",
                   model="./models/facebook/blenderbot-400M-distill")
				   
	user_message = """
	What are some fun activities I can do in the winter?
	"""			

	from transformers import Conversation

	conversation = Conversation(user_message)

    print(conversation)

	conversation = chatbot(conversation)

    print(conversation)	
	
You can continue the conversation with the chatbot with:

	print(chatbot(Conversation("What else do you recommend?")))

However, the chatbot may provide an unrelated response because it does not have memory of any prior conversations.

To include prior conversations in the LLM's context, you can add a 'message' to include the previous chat history.

	conversation.add_message(
    {"role": "user",
     "content": """
		What else do you recommend?
		"""
    })
	
	print(conversation)
	
	conversation = chatbot(conversation)

	print(conversation)
	
	
LESSON 3 

Translation and Summazation 

	!pip install transformers 
    !pip install torch
	 
	from transformers.utils import logging
	logging.set_verbosity_error()
	
Build the translation pipeline using ðŸ¤— Transformers Library
	from transformers import pipeline 
	import torch
	
	translator = pipeline(task="translation",
                      model="./models/facebook/nllb-200-distilled-600M",
                      torch_dtype=torch.bfloat16) 
					  
NLLB: No Language Left Behind: 'nllb-200-distilled-600M'.
	text = """\
		My puppy is adorable, \
		Your kitten is cute.
		Her panda is friendly.
		His llama is thoughtful. \
		We all have nice pets!"""					
	
	text_translated = translator(text,
                             src_lang="eng_Latn",
                             tgt_lang="fra_Latn")	
							 
Free up some memory before continuing

	import gc
    del translator
	gc.collect()

Build the summarization pipeline using ðŸ¤— Transformers Library 	
	summarizer = pipeline(task="summarization",
                      model="./models/facebook/bart-large-cnn",
                      torch_dtype=torch.bfloat16)
					  
	text = """Paris is the capital and most populous city of France, with
          an estimated population of 2,175,601 residents as of 2018,
          in an area of more than 105 square kilometres (41 square
          miles). The City of Paris is the centre and seat of
          government of the region and province of ÃŽle-de-France, or
          Paris Region, which has an estimated population of
          12,174,880, or about 18 percent of the population of France
          as of 2017."""

	summary = summarizer(text,
                     min_length=10,
                     max_length=100)

	summary				 
	
	

LEESON 4 

	!pip install sentence-transformers
	
	from transformers.utils import logging
	logging.set_verbosity_error()
	
Build the sentence embedding pipeline using ðŸ¤— Transformers Library
	from sentence_transformers import SentenceTransformer
	model = SentenceTransformer("all-MiniLM-L6-v2")	
	
	sentences1 = ['The cat sits outside',
              'A man is playing guitar',
              'The movies are awesome']
			  
	embeddings1 = model.encode(sentences1, convert_to_tensor=True)		  
	
	embeddings1 
	
	sentences2 = ['The dog plays in the garden',
              'A woman watches TV',
              'The new movie is so great']
			  
	embeddings2 = model.encode(sentences2, 
                           convert_to_tensor=True

	print(embeddings2)

Calculate the cosine similarity between two sentences as a measure of how similar they are to each other.	

	from sentence_transformers import util
	cosine_scores = util.cos_sim(embeddings1,embeddings2)
	print(cosine_scores)
	for i in range(len(sentences1)):
    print("{} \t\t {} \t\t Score: {:.4f}".format(sentences1[i],
                                                 sentences2[i],
                                                 cosine_scores[i][i])
												 
												 
LESSON 5 

In the classroom, the libraries have already been installed for you.
If you are running this code on your own machine, please install the following

	!pip install transformers
	!pip install datasets
	!pip install soundfile
	!pip install librosa
	
The librosa library may need to have ffmpeg installed.
This page on librosa provides installation instructions for ffmpeg.	

	from transformers.utils import logging
	logging.set_verbosity_error()	
												 
	
Prepare the dataset of audio recordings
	from datasets import load_dataset, load_from_disk

	# This dataset is a collection of different sounds of 5 seconds
	# dataset = load_dataset("ashraq/esc50",
	#                       split="train[0:10]")
	dataset = load_from_disk("./models/ashraq/esc50/train")
	
	audio_sample = dataset[0]
	audio_sample
	
	from IPython.display import Audio as IPythonAudio
	IPythonAudio(audio_sample["audio"]["array"],
             rate=audio_sample["audio"]["sampling_rate"])
			 
Build the audio classification pipeline using ðŸ¤— Transformers Library		
	from transformers import pipeline	
    zero_shot_classifier = pipeline(
    task="zero-shot-audio-classification",
    model="./models/laion/clap-htsat-unfused") 
	
Sampling Rate for Transformer Models
How long does 1 second of high resolution audio (192,000 Hz) appear to the Whisper model (which is trained to expect audio files at 16,000 Hz)?	
	(1 * 192000) / 16000
	
	Sampling Rate for Transformer Models
	How long does 1 second of high resolution audio (192,000 Hz) appear to the Whisper model (which is trained to expect audio files at 16,000 Hz)?
	
	The 1 second of high resolution audio appears to the model as if it is 12 seconds of audio.
	How about 5 seconds of audio?
	(5 * 192000) / 16000
	
	(5 * 192000) / 16000
	
	zero_shot_classifier.feature_extractor.sampling_rate
	audio_sample["audio"]["sampling_rate"]
	
Set the correct sampling rate for the input and the model.

    from datasets import Audio
	
	dataset = dataset.cast_column(
    "audio",
     Audio(sampling_rate=48_000))
	 
	audio_sample = dataset[0] 

	audio_sample

    candidate_labels = ["Sound of a dog",
                    "Sound of vacuum cleaner"]

	zero_shot_classifier(audio_sample["audio"]["array"],
                     candidate_labels=candidate_labels)	

	candidate_labels = ["Sound of a child crying",
                    "Sound of vacuum cleaner",
                    "Sound of a bird singing",
                    "Sound of an airplane"]	

	zero_shot_classifier(audio_sample["audio"]["array"],
                     candidate_labels=candidate_labels)	

LESSON 6
Automatic Speech Recognition	
Using OpenAI



  !pip install transformers
  !pip install -U datasets
  !pip install soundfile
  !pip install librosa
  !pip install gradio

	
	
   from transformers.utils import logging
   logging.set_verbosity_error()
   
Data preparation
   
   from datasets import load_dataset
   
   dataset = load_dataset("librispeech_asr",
                       split="train.clean.100",
                       streaming=True,
                       trust_remote_code=True)
					   
	example = next(iter(dataset))

	dataset_head = dataset.take(5)
    list(dataset_head)
	
	list(dataset_head)[2]
	
	example
	
	from IPython.display import Audio as IPythonAudio

	IPythonAudio(example["audio"]["array"],
             rate=example["audio"]["sampling_rate"])
			 
Build the pipeline	

    from transformers import pipeline

    asr = pipeline(task="automatic-speech-recognition",
               model="./models/distil-whisper/distil-small.en")
			   
	asr.feature_extractor.sampling_rate		   
	
	example['audio']['sampling_rate']
	
	asr(example["audio"]["array"])
	
	example["text"]
	
Build a shareable app with Gradio

	import os
    import gradio as gr
	
	demo = gr.Blocks()
	
	def transcribe_speech(filepath):
    if filepath is None:
        gr.Warning("No audio found, please retry.")
        return ""
    output = asr(filepath)
    return output["text"]
	
	mic_transcribe = gr.Interface(
    fn=transcribe_speech,
    inputs=gr.Audio(sources="microphone",
                    type="filepath"),
    outputs=gr.Textbox(label="Transcription",
                       lines=3),
    allow_flagging="never")
	
	
	
	file_transcribe = gr.Interface(
    fn=transcribe_speech,
    inputs=gr.Audio(sources="upload",
                    type="filepath"),
    outputs=gr.Textbox(label="Transcription",
                       lines=3),
    allow_flagging="never",)
	
	with demo:
    gr.TabbedInterface(
        [mic_transcribe,
         file_transcribe],
        ["Transcribe Microphone",
         "Transcribe Audio File"],
    )

	demo.launch(share=True, 
            server_port=int(os.environ['PORT1']))
			
	demo.close()
	
	
Note: Please stop the demo before continuing with the rest of the lab.

	demo.close()
	OSError: Cannot find empty port in range


	import soundfile as sf
	import io
	
	audio, sampling_rate = sf.read('narration_example.wav')
	
	sampling_rate
	
	asr.feature_extractor.sampling_rate
	
	asr(audio)
	ValueError: We expect a single channel audio input for AutomaticSpeechRecognitionPipeline
	
	
	audio.shape
	
	import numpy as np

    audio_transposed = np.transpose(audio)
	
	audio_transposed.shape
	
	import librosa
	
	audio_mono = librosa.to_mono(audio_transposed)
	
	IPythonAudio(audio_mono,
             rate=sampling_rate)
			 
	asr(audio_mono)

	sampling_rate
	
	asr.feature_extractor.sampling_rate
	
	audio_16KHz = librosa.resample(audio_mono,
                               orig_sr=sampling_rate,
                               target_sr=16000)
							   
	asr(
    audio_16KHz,
    chunk_length_s=30, # 30 seconds
    batch_size=4,
    return_timestamps=True,
	)["chunks"]	

Build the Gradio interface.

	import gradio as gr
    demo = gr.Blocks()
	
	
	def transcribe_long_form(filepath):
    if filepath is None:
        gr.Warning("No audio found, please retry.")
        return ""
    output = asr(
      filepath,
      max_new_tokens=256,
      chunk_length_s=30,
      batch_size=8,
    )
    return output["text"]
	
	mic_transcribe = gr.Interface(
    fn=transcribe_long_form,
    inputs=gr.Audio(sources="microphone",
                    type="filepath"),
    outputs=gr.Textbox(label="Transcription",
                       lines=3),
    allow_flagging="never")

	file_transcribe = gr.Interface(
		fn=transcribe_long_form,
		inputs=gr.Audio(sources="upload",
						type="filepath"),
		outputs=gr.Textbox(label="Transcription",
						   lines=3),
		allow_flagging="never",
	)
	
	with demo:
    gr.TabbedInterface(
        [mic_transcribe,
         file_transcribe],
        ["Transcribe Microphone",
         "Transcribe Audio File"],
    )
	demo.launch(share=True, 
            server_port=int(os.environ['PORT1']))
			
	demo.close()		
	
Note: Please stop the demo before continuing with the rest of the lab.	
	
	demo.close()
	If you run another gradio app (later in this lesson) without first closing this appp, you'll see an error message:
	
	OSError: Cannot find empty port in range
	
	Testing with a longer audio file
	
	import soundfile as sf
    import io
	
	audio, sampling_rate = sf.read('narration_example.wav')
	
	sampling_rate
	
	asr.feature_extractor.sampling_rate
	
	asr(audio)
	
	ValueError: We expect a single channel audio input for AutomaticSpeechRecognitionPipeline
	
	
Note: Please stop the demo before continuing with the rest of the lab.

    demo.close()
	
	OSError: Cannot find empty port in range
	
Try it yourself!

	import soundfile as sf
	import io

	audio, sampling_rate = sf.read('narration_example.wav')	
	
	sampling_rate
	asr.feature_extractor.sampling_rate
	
	
Lesson 7: Text to Speech
		
	
In the classroom, the libraries are already installed for you.
If you would like to run this code on your own machine, you can install the following:
    !pip install transformers
    !pip install gradio
    !pip install timm
    !pip install timm
    !pip install inflect
    !pip install phonemizer	
	
Note: py-espeak-ng is only available Linux operating systems.

To run locally in a Linux machine, follow these commands:

    sudo apt-get update
    sudo apt-get install espeak-ng
    pip install py-espeak-ng	
	
Build the text-to-speech pipeline using the ðŸ¤— Transformers Library

	
Here is some code that suppresses warning messages

	from transformers.utils import logging

	logging.set_verbosity_error()
	from transformers import pipeline

	narrator = pipeline("text-to-speech",
                    model="./models/kakao-enterprise/vits-ljs")
	
	
	text = """
	Researchers at the Allen Institute for AI, \
	HuggingFace, Microsoft, the University of Washington, \
	Carnegie Mellon University, and the Hebrew University of \
	Jerusalem developed a tool that measures atmospheric \
	carbon emitted by cloud servers while training machine \
	learning models. After a modelâ€™s size, the biggest variables \
	were the serverâ€™s location and time of day it was active.
	"""
	
	narrated_text = narrator(text)
	from IPython.display import Audio as IPythonAudio

	IPythonAudio(narrated_text["audio"][0],
             rate=narrated_text["sampling_rate"])
	
	
	
Lesson 8: Object Detection

 In the classroom, the libraries are already installed for you.
If you would like to run this code on your own machine, you can install the following:
    !pip install transformers
    !pip install gradio
    !pip install timm
    !pip install inflect
    !pip install phonemizer
	
Note: py-espeak-ng is only available Linux operating systems.

To run locally in a Linux machine, follow these commands:

    sudo apt-get update
    sudo apt-get install espeak-ng
    pip install py-espeak-ng

Build the object-detection pipeline using ðŸ¤— Transformers Library

	from helper import load_image_from_url, render_results_in_image
	from transformers import pipeline
	
	from transformers.utils import logging
	logging.set_verbosity_error()

	from helper import ignore_warnings
	ignore_warnings()
	
	od_pipe = pipeline("object-detection", "./models/facebook/detr-resnet-50")
	
	Info about facebook/detr-resnet-50

	Explore more of the Hugging Face Hub for more object detection models
	
Use the Pipeline
	
	from PIL import Image
	raw_image = Image.open('huggingface_friends.jpg')
raw_image.resize((569, 491))

    pipeline_output = od_pipe(raw_image)
	
Return the results from the pipeline using the helper function render_results_in_image.

    processed_image = render_results_in_image(
    raw_image, 
    pipeline_output)
	
	processed_image
	
Using `Gradio` as a Simple Interface

- Use [Gradio](https://www.gradio.app) to create a demo for the object detection app.
- The demo makes it look friendly and easy to use.
- You can share the demo with your friends and colleagues as well.

	import os
	import gradio as gr

		def get_pipeline_prediction(pil_image):
		
		pipeline_output = od_pipe(pil_image)
		
		processed_image = render_results_in_image(pil_image,
												pipeline_output)
		return processed_image

	
	demo = gr.Interface(
	  fn=get_pipeline_prediction,
	  inputs=gr.Image(label="Input image", 
					  type="pil"),
	  outputs=gr.Image(label="Output image with predicted instances",
					   type="pil")
	)
	
	share=True will provide an online link to access to the demo
	
	demo.launch(share=True, server_port=int(os.environ['PORT1']))
	
	demo.close(
	
	

Close the app


Remember to call `.close()` on the Gradio app when you're done using it.	

Make an AI Powered Audio Assistant

- Combine the object detector with a text-to-speech model that will help dictate what is inside the image.

- Inspect the output of the object detection pipeline.	

	pipeline_output
	
	od_pipe
	
	raw_image = Image.open('huggingface_friends.jpg')
	raw_image.resize((284, 245))
	
	from helper import summarize_predictions_natural_language
	
	text = summarize_predictions_natural_language(pipeline_output)
	
	text
	
Generate Audio Narration of an Image

	tts_pipe = pipeline("text-to-speech",
                    model="./models/kakao-enterprise/vits-ljs")
					
More info about kakao-enterprise/vits-ljs.

	narrated_text = tts_pipe(text)	


Play the Generated Audio

	from IPython.display import Audio as IPythonAudio
	IPythonAudio(narrated_text["audio"][0],
             rate=narrated_text["sampling_rate"])
	
	

					

In the classroom, the libraries are already installed for you.
If you would like to run this code on your own machine, you can install the following:
    !pip install transformers
    !pip install gradio
    !pip install timm
    !pip install torchvision	
	
	
	
Lesson 9: Segmentation

- In the classroom, the libraries are already installed for you.
- If you would like to run this code on your own machine, you can install the following:

    !pip install transformers
    !pip install gradio
    !pip install timm
    !pip install torchvision
	
	
Here is some code that suppresses warning messages.

	from transformers.utils import logging
	logging.set_verbosity_error()
	
	
Mask Generation with SAM

The [Segment Anything Model (SAM)](https://segment-anything.com) model was released by Meta AI.	

	from transformers import pipeline
	sam_pipe = pipeline("mask-generation",
    "./models/Zigeng/SlimSAM-uniform-77")
	
	from PIL import Image
	
	raw_image = Image.open('meta_llamas.jpg')
    raw_image.resize((720, 375))
	
Running this will take some time
The higher the value of 'points_per_batch', the more efficient pipeline inference will be
	
	output = sam_pipe(raw_image, points_per_batch=32)
    from helper import show_pipe_masks_on_image	
	
	show_pipe_masks_on_image(raw_image, output)
	
Note: The colors of segmentation, that you will get when running this code, might be different than the ones you see in the video.	


Faster Inference: Infer an Image and a Single Point

	from transformers import SamModel, SamProcessor
	
	model = SamModel.from_pretrained(
    "./models/Zigeng/SlimSAM-uniform-77")

	processor = SamProcessor.from_pretrained(
		"./models/Zigeng/SlimSAM-uniform-77")
		
	raw_image.resize((720, 375))

Segment the blue shirt Andrew is wearing.
Give any single 2D point that would be in that region (blue shirt).

	
	input_points = [[[1600, 700]]]
	
Create the input using the image and the single point.
return_tensors="pt" means to return PyTorch Tensors.	

	inputs = processor(raw_image,
                 input_points=input_points,
                 return_tensors="pt")
	
	
Given the inputs, get the output from the model.

	import torch
	
	with torch.no_grad():
    outputs = model(**inputs)
	
	predicted_masks = processor.image_processor.post_process_masks(
    outputs.pred_masks,
    inputs["original_sizes"],
    inputs["reshaped_input_sizes"])
	
Length of predicted_masks corresponds to the number of images that are used in the input

	len(predicted_masks)
	
	predicted_mask = predicted_masks[0]
    predicted_mask.shape
	
	outputs.iou_scores
	
	from helper import show_mask_on_image
	
	for i in range(3):
    show_mask_on_image(raw_image, predicted_mask[:, i])
	
Depth Estimation with DPT
 
This model was introduced in the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al. (2021) and first released in [isl-org/DPT](https://github.com/isl-org/DPT).


	depth_estimator = pipeline(task="depth-estimation",
                        model="./models/Intel/dpt-hybrid-midas"
						
	raw_image = Image.open('gradio_tamagochi_vienna.png')
	
If you'd like to generate this image or something like it, check out the short course on [Gradio](https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/) and go to the lesson "Image Generation App".

	output = depth_estimator(raw_image)
	
	output

		
Post-process the output image to resize it to the size of the original image.

	output["predicted_depth"].shape
	
	output["predicted_depth"].unsqueeze(1).shape
	
	prediction = torch.nn.functional.interpolate(
    output["predicted_depth"].unsqueeze(1),
    size=raw_image.size[::-1],
    mode="bicubic",
    align_corners=False,)
	
	prediction.shape
	
	raw_image.size[::-1],
	
	prediction
	
Normalize the predicted tensors (between 0 and 255) so that they can be displayed.

	
	import numpy as np 
		
	output = prediction.squeeze().numpy()
	formatted = (output * 255 / np.max(output)).astype("uint8")
	depth = Image.fromarray(formatted)
	
	depth
	
Demo using Gradio

Troubleshooting Tip
Note, in the classroom, you may see the code for creating the Gradio app run indefinitely.
This is specific to this classroom environment when it's serving many learners at once, and you won't wouldn't experience this issue if you run this code on your own machine.
To fix this, please restart the kernel (Menu Kernel->Restart Kernel) and re-run the code in the lab from the beginning of the lesson

	import os
	import gradio as gr
	from transformers import pipeline
	
	def launch(input_image):
    out = depth_estimator(input_image)

    # resize the prediction
    prediction = torch.nn.functional.interpolate(
        out["predicted_depth"].unsqueeze(1),
        size=input_image.size[::-1],
        mode="bicubic",
        align_corners=False,
    )

    # normalize the prediction
    output = prediction.squeeze().numpy()
    formatted = (output * 255 / np.max(output)).astype("uint8")
    depth = Image.fromarray(formatted)
    return depth
	
	iface = gr.Interface(launch, 
                     inputs=gr.Image(type='pil'), 
                     outputs=gr.Image(type='pil'))
					 
	iface.launch(share=True, server_port=int(os.environ['PORT1']))

	iface.close()

Close the app
	Remember to call .close() on the Gradio app when you're done using it.	
	
	
	
 
	
	
	
	
	
	
	

	
				 